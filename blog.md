# Activities Blog for the court hearing on 5/6/2022 (without any proprietary source code)

For the last ~5 years I have been focusing on deep learning for NLP (natural language processing). The circumstantial reasons I chose this topic are eloquently described [here](http://karpathy.github.io/2022/03/14/lecun1989/).

I have been particularly fascinated by the "transformers" idea. Since implemented deep learning could be summarized as “endless addition and multiplication of grouped numbers," the attention mechanism of transformers cleverly emphasize the effects of certain inputs over others throughout this process (more [here](https://www.quantamagazine.org/will-transformers-take-over-artificial-intelligence-20220310/)).

I have no computing resources to attempt to effectively train any non-trivial transformer. Fortunately, [Hugging Face](https://huggingface.co) has collected and systematically catalogued a great wealth of already trained datasets.

Open source implementations of the various transformer models are also provided in the Hugging Face git repositories (unlike the API based approach like [this](https://openai.com/blog/customized-gpt-3/)). Reading the well documented source code aids in initial efforts to master the technology.

Once familiarity is established, however, patterns of usage, and specifically the differences between approaches, become essential. While feverishly experimenting with different ideas, I personally found distracting to look at the same algorithms written inconsistently, with differing naming conventions, etc. Scrolling through long function definitions also broke my “flow.”

Through the years I found that my own summary implementations of the most fundamental “library” features greatly enhanced my understanding and, most importantly, my speed to achieve results.

Therefore, I have decided to spend the 2-3 months the courts are intently monitoring me on contributing to the open source “transformers” effort. My contribution is a consistent, significantly simplified and compact (to intently minimize cognitive overload) re-write of the open source transformers codebase.

Once again, my objective has been to create easily recognizable “code patterns” for quick development feedback loops.

## 3/16/2022

TBD

## 3/23/2022

TBD
